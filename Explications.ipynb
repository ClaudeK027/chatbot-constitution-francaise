{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture du Chatbot RAG pour la Constitution française\n",
    "\n",
    "## Vue d'ensemble\n",
    "\n",
    "Nous implémentons un chatbot basé sur la technique de Retrieval Augmented Generation (RAG) pour répondre aux questions sur la Constitution française. L'architecture se compose de plusieurs modules interconnectés, chacun responsable d'une partie spécifique du processus.\n",
    "\n",
    "## Composants principaux\n",
    "\n",
    "1. **PDF Processor (`pdf_processor.py`)**\n",
    "   - Responsable du chargement et de la segmentation du PDF de la Constitution.\n",
    "   - Utilise `PyPDFLoader` de Langchain pour charger le PDF.\n",
    "   - Implémente une logique personnalisée pour diviser le texte en chunks structurés (préambule, titres, articles).\n",
    "\n",
    "2. **Embedding Generator (`embedding_generator.py`)**\n",
    "   - Génère des embeddings pour les chunks de texte.\n",
    "   - Utilise le modèle `BAAI/bge-large-en-v1.5` de SentenceTransformer pour la génération d'embeddings.\n",
    "   - Stocke les embeddings dans une base de données vectorielle ChromaDB.\n",
    "\n",
    "3. **Semantic Search (`semantic_search.py`)**\n",
    "   - Effectue des recherches sémantiques dans la base de données d'embeddings.\n",
    "   - Utilise ChromaDB pour la recherche rapide des chunks les plus pertinents.\n",
    "\n",
    "4. **LLM Interface (`llm_interface.py`)**\n",
    "   - Fournit une interface pour interagir avec les modèles de langage (LLM).\n",
    "   - Utilise une API locale (probablement Ollama) pour accéder aux modèles Mistral et Phi-2.\n",
    "\n",
    "5. **Main Script (`main.py`)**\n",
    "   - Sert d'interface utilisateur et d'orchestrateur pour le chatbot.\n",
    "   - Gère le flux de travail complet, de la réception de la question à la génération de la réponse.\n",
    "\n",
    "## Flux de travail\n",
    "\n",
    "1. **Prétraitement (effectué une seule fois)**\n",
    "   - Le PDF est chargé et divisé en chunks structurés.\n",
    "   - Les embeddings sont générés pour chaque chunk et stockés dans ChromaDB.\n",
    "\n",
    "2. **Processus de réponse aux questions**\n",
    "   - L'utilisateur pose une question via l'interface console.\n",
    "   - La question est utilisée pour effectuer une recherche sémantique dans ChromaDB.\n",
    "   - Les chunks les plus pertinents sont récupérés.\n",
    "   - Un prompt est construit en combinant la question et les chunks pertinents.\n",
    "   - Le prompt est envoyé au LLM via l'interface LLM.\n",
    "   - La réponse générée par le LLM est affichée à l'utilisateur, avec les sources utilisées.\n",
    "\n",
    "## Caractéristiques clés\n",
    "\n",
    "- **Segmentation intelligente** : Le PDF est divisé de manière à préserver la structure de la Constitution (préambule, titres, articles).\n",
    "- **Recherche sémantique avancée** : Utilisation d'un modèle d'embedding performant pour une recherche précise.\n",
    "- **Flexibilité du LLM** : Possibilité de choisir entre différents modèles (Mistral, Phi-2).\n",
    "- **Contextualisation** : La réponse est générée en tenant compte du contexte spécifique de la Constitution.\n",
    "- **Traçabilité** : Les sources utilisées pour générer la réponse sont fournies à l'utilisateur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Chargement et prétraitement du pdf : `pdf_processor.py`\n",
    "\n",
    "### Classe `PDFProcessor`\n",
    "- **Initialisation** : Utilise `PyPDFLoader` de Langchain pour charger le PDF.\n",
    "- **Méthode `load_and_split()`** :\n",
    "  - Charge le PDF et extrait le texte complet.\n",
    "  - Appelle `split_by_structure()` pour segmenter le texte.\n",
    "- **Méthode `split_by_structure(text)`** :\n",
    "  - Utilise des expressions régulières pour diviser le texte en chunks structurés.\n",
    "  - Extrait le préambule (avant l'Article 1).\n",
    "  - Divise le reste en titres et articles.\n",
    "  - Crée des objets `Document` avec métadonnées (titre, numéro d'article).\n",
    "- **Méthode `process()`** :\n",
    "  - Point d'entrée principal qui appelle `load_and_split()`.\n",
    "  - Affiche un aperçu des chunks pour vérification.\n",
    "\n",
    "**Techniques clés** :\n",
    "- Utilisation d'expressions régulières pour une segmentation précise.\n",
    "- Conservation de la structure du document (titres, articles) dans les métadonnées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le document a été découpé en 79 chunks.\n",
      "\n",
      "Chunk 0:\n",
      "Titre: Préambule\n",
      "Article: 0\n",
      "1 janvier 2015  \n",
      " \n",
      " \n",
      "CONSTITUTION  \n",
      " \n",
      " \n",
      " \n",
      "Le Gouvernement de la République, conformément \n",
      "à la loi constitutionnelle du 3  juin 1958, a proposé,  \n",
      " Le peuple français a adopté,  \n",
      " \n",
      "Le Président de la ...\n",
      "\n",
      "Chunk 1:\n",
      "Titre: Titre II \n",
      "LE PRÉSIDENT DE LA R ÉPUBLIQUE\n",
      "Article: 5\n",
      "ARTICLE 5.\n",
      "Le Président de la République veille au respect de la Co nstitution. Il \n",
      "assure, par son arbitrage, le fonctionnement régulier des pouvoirs publics ainsi que \n",
      "la continuité de l'État.  \n",
      "Il ...\n",
      "\n",
      "Chunk 2:\n",
      "Titre: Titre II \n",
      "LE PRÉSIDENT DE LA R ÉPUBLIQUE\n",
      "Article: 6\n",
      "ARTICLE 6.\n",
      "Le Président de la République est élu pour cinq ans au suffrage \n",
      "universel direct.  \n",
      "Nul ne peut exercer plus de deux mandats consécutifs.  \n",
      "Les modalités d'application du présent article s...\n",
      "\n",
      "Chunk 3:\n",
      "Titre: Titre II \n",
      "LE PRÉSIDENT DE LA R ÉPUBLIQUE\n",
      "Article: 7\n",
      "ARTICLE 7.\n",
      "Le Président de la République est é lu à la majorité absolue des \n",
      "suffrages exprimés. Si celle -ci n'est pas obtenue au premier tour de scrutin, il est \n",
      "procédé, le quatorzième jour suivant...\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "class PDFProcessor:\n",
    "    def __init__(self, pdf_path):\n",
    "        self.pdf_path = pdf_path\n",
    "        self.loader = PyPDFLoader(pdf_path)\n",
    "\n",
    "    def load_and_split(self):\n",
    "        pages = self.loader.load_and_split()\n",
    "        full_text = ' '.join([page.page_content for page in pages])\n",
    "        chunks = self.split_by_structure(full_text)\n",
    "        return chunks\n",
    "\n",
    "    def split_by_structure(self, text):\n",
    "        chunks = []\n",
    "        \n",
    "        # Extraire le contenu avant l'Article 1 (Chunk 0)\n",
    "        pre_article_1 = re.split(r'ARTICLE\\s+PREMIER\\.?', text)[0]\n",
    "        chunks.append(Document(\n",
    "            page_content=pre_article_1.strip(),\n",
    "            metadata={\n",
    "                \"source\": self.pdf_path,\n",
    "                \"title\": \"Préambule\",\n",
    "                \"article_num\": \"0\"\n",
    "            }\n",
    "        ))\n",
    "\n",
    "        # Diviser le reste du texte en titres et articles\n",
    "        main_text = text[len(pre_article_1):]\n",
    "        title_pattern = r'Titre\\s+[IVX]+\\s*\\n([^\\n]+)'\n",
    "        article_pattern = r'ARTICLE\\s+(\\w+(?:\\-\\d+)?)\\.?([^A]+)(?=ARTICLE|$)'\n",
    "        \n",
    "        current_title = \"\"\n",
    "        for title_match in re.finditer(title_pattern, main_text):\n",
    "            title = title_match.group(0).strip()\n",
    "            current_title = title\n",
    "            title_end = title_match.end()\n",
    "            next_title_match = re.search(title_pattern, main_text[title_end:])\n",
    "            title_content = main_text[title_end:next_title_match.start() + title_end] if next_title_match else main_text[title_end:]\n",
    "            \n",
    "            for article_match in re.finditer(article_pattern, title_content):\n",
    "                article_num = article_match.group(1)\n",
    "                article_content = article_match.group(2).strip()\n",
    "                \n",
    "                chunk = Document(\n",
    "                    page_content=f\"ARTICLE {article_num}.\\n{article_content}\",\n",
    "                    metadata={\n",
    "                        \"source\": self.pdf_path,\n",
    "                        \"title\": current_title,\n",
    "                        \"article_num\": article_num\n",
    "                    }\n",
    "                )\n",
    "                chunks.append(chunk)\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    def process(self):\n",
    "        chunks = self.load_and_split()\n",
    "        print(f\"Le document a été découpé en {len(chunks)} chunks.\")\n",
    "        \n",
    "        # Afficher un aperçu des chunks pour vérification\n",
    "        for i, chunk in enumerate(chunks[:4]):  # Afficher les 10 premiers chunks\n",
    "            print(f\"\\nChunk {i}:\")\n",
    "            print(f\"Titre: {chunk.metadata['title']}\")\n",
    "            print(f\"Article: {chunk.metadata['article_num']}\")\n",
    "            print(chunk.page_content[:200] + \"...\")  # Afficher les 200 premiers caractères\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "# Exemple d'utilisation\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = \"constitution.pdf\"\n",
    "    processor = PDFProcessor(pdf_path)\n",
    "    chunks = processor.process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extraction et stockage de embeddings : `embedding_generator.py`\n",
    "\n",
    "### Classe `EmbeddingGenerator`\n",
    "- **Initialisation** : \n",
    "  - Charge le modèle SentenceTransformer 'BAAI/bge-large-en-v1.5'.\n",
    "  - Configure ChromaDB pour le stockage persistant.\n",
    "- **Méthode `generate_embeddings(chunks)`** :\n",
    "  - Génère des embeddings pour les chunks de texte.\n",
    "  - Applique une normalisation L2 aux embeddings.\n",
    "- **Méthode `store_embeddings(chunks, embeddings)`** :\n",
    "  - Stocke les embeddings, textes et métadonnées dans ChromaDB.\n",
    "- **Méthode `process_and_store(chunks)`** :\n",
    "  - Combine la génération et le stockage des embeddings.\n",
    "- **Méthode `get_similar_chunks(query, n_results)`** :\n",
    "  - Effectue une recherche sémantique dans ChromaDB.\n",
    "- **Méthode `update_embeddings(chunks)`** :\n",
    "  - Met à jour les embeddings pour les chunks existants.\n",
    "\n",
    "**Techniques clés** :\n",
    "- Utilisation de SentenceTransformer pour la génération d'embeddings de haute qualité.\n",
    "- Normalisation L2 pour améliorer la qualité des embeddings.\n",
    "- Intégration avec ChromaDB pour un stockage et une recherche efficaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kaizen\\Desktop\\Test_peaks\\chatbot_env\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "import os\n",
    "import numpy as np\n",
    "from typing import List, Dict\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "class EmbeddingGenerator:\n",
    "    def __init__(self, model_name='BAAI/bge-large-en-v1.5'):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.persist_directory = os.path.join(os.getcwd(), \"chroma_db\")\n",
    "        self.chroma_client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "        self.collection = self.chroma_client.get_or_create_collection(\"document_embeddings\")\n",
    "\n",
    "    def generate_embeddings(self, chunks: List[Dict]):\n",
    "        texts = [chunk.page_content for chunk in chunks]\n",
    "        \n",
    "        # Générer des embeddings avec SentenceTransformer\n",
    "        embeddings = self.model.encode(texts, show_progress_bar=True)\n",
    "        \n",
    "        # Normalisation L2 des embeddings\n",
    "        embeddings = normalize(embeddings)\n",
    "        \n",
    "        return embeddings\n",
    "\n",
    "    def store_embeddings(self, chunks: List[Dict], embeddings: np.ndarray):\n",
    "        ids = [str(i) for i in range(len(chunks))]\n",
    "        metadatas = [\n",
    "            {\n",
    "                \"source\": chunk.metadata.get('source', ''),\n",
    "                \"title\": chunk.metadata.get('title', ''),\n",
    "                \"article_num\": chunk.metadata.get('article_num', '')\n",
    "            } \n",
    "            for chunk in chunks\n",
    "        ]\n",
    "        texts = [chunk.page_content for chunk in chunks]\n",
    "        \n",
    "        self.collection.add(\n",
    "            embeddings=embeddings.tolist(),\n",
    "            documents=texts,\n",
    "            metadatas=metadatas,\n",
    "            ids=ids\n",
    "        )\n",
    "\n",
    "    def process_and_store(self, chunks: List[Dict]):\n",
    "        embeddings = self.generate_embeddings(chunks)\n",
    "        self.store_embeddings(chunks, embeddings)\n",
    "        print(f\"Generated and stored embeddings for {len(chunks)} chunks.\")\n",
    "\n",
    "    def get_collection_info(self):\n",
    "        return {\n",
    "            \"name\": self.collection.name,\n",
    "            \"count\": self.collection.count()\n",
    "        }\n",
    "\n",
    "    def get_similar_chunks(self, query: str, n_results: int = 5):\n",
    "        query_embedding = self.generate_embeddings([{\"page_content\": query}])\n",
    "        results = self.collection.query(\n",
    "            query_embeddings=query_embedding.tolist(),\n",
    "            n_results=n_results,\n",
    "            include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "        )\n",
    "        return results\n",
    "\n",
    "    def update_embeddings(self, chunks: List[Dict]):\n",
    "        \"\"\"Mise à jour des embeddings pour les chunks existants.\"\"\"\n",
    "        embeddings = self.generate_embeddings(chunks)\n",
    "        ids = [str(i) for i in range(len(chunks))]\n",
    "        self.collection.update(\n",
    "            ids=ids,\n",
    "            embeddings=embeddings.tolist(),\n",
    "            metadatas=[chunk.metadata for chunk in chunks],\n",
    "            documents=[chunk.page_content for chunk in chunks]\n",
    "        )\n",
    "        print(f\"Updated embeddings for {len(chunks)} chunks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Recherche semantique à partir d'une entrée : `semantic_search.py`\n",
    "\n",
    "### Classe `SemanticSearch`\n",
    "- **Initialisation** :\n",
    "  - Configure le modèle SentenceTransformer et la connexion à ChromaDB.\n",
    "- **Méthode `search(query, n_results)`** :\n",
    "  - Génère l'embedding de la requête.\n",
    "  - Effectue une recherche dans ChromaDB.\n",
    "  - Formate les résultats avec des scores de pertinence.\n",
    "- **Méthode `get_relevant_context(query, max_length)`** :\n",
    "  - Récupère et concatène les chunks pertinents jusqu'à une longueur maximale.\n",
    "\n",
    "**Techniques clés** :\n",
    "- Utilisation de la similitude cosinus pour le classement des résultats.\n",
    "- Limitation de la longueur du contexte pour optimiser l'entrée du LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import os\n",
    "\n",
    "class SemanticSearch:\n",
    "    def __init__(self, model_name='BAAI/bge-large-en-v1.5'):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.persist_directory = os.path.join(os.getcwd(), \"chroma_db\")\n",
    "        self.chroma_client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "        self.collection = self.chroma_client.get_collection(\"document_embeddings\")\n",
    "\n",
    "    def search(self, query, n_results=5):\n",
    "        # Générer l'embedding pour la requête\n",
    "        query_embedding = self.model.encode(query).tolist()\n",
    "\n",
    "        # Effectuer la recherche dans ChromaDB\n",
    "        results = self.collection.query(\n",
    "            query_embeddings=[query_embedding],\n",
    "            n_results=n_results,\n",
    "            include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "        )\n",
    "\n",
    "        # Formater les résultats\n",
    "        formatted_results = []\n",
    "        for doc, metadata, distance in zip(results['documents'][0], results['metadatas'][0], results['distances'][0]):\n",
    "            formatted_results.append({\n",
    "                'content': doc,\n",
    "                'metadata': metadata,\n",
    "                'relevance_score': 1 - distance  # Convertir la distance en score de pertinence\n",
    "            })\n",
    "\n",
    "        return formatted_results\n",
    "\n",
    "    def get_relevant_context(self, query, max_length=1000):\n",
    "        results = self.search(query)\n",
    "        context = \"\"\n",
    "        for result in results:\n",
    "            if len(context) + len(result['content']) <= max_length:\n",
    "                context += result['content'] + \" \"\n",
    "            else:\n",
    "                break\n",
    "        return context.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Interraction avec le modèle Mistral 7B(Ollama) : `llm_interface.py`\n",
    "\n",
    "### Classe `LLMInterface`\n",
    "- **Initialisation** : Configure l'URL de l'API locale (probablement Ollama).\n",
    "- **Méthode `generate_response(prompt, max_tokens)`** :\n",
    "  - Envoie une requête à l'API locale du LLM.\n",
    "  - Gère les erreurs de connexion.\n",
    "- **Méthode `simple_query(question)`** :\n",
    "  - Formate une question simple pour le LLM.\n",
    "\n",
    "**Techniques clés** :\n",
    "- Utilisation d'une API locale pour l'inférence du LLM.\n",
    "- Gestion des erreurs pour une meilleure robustesse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "m1 = 'mistral'\n",
    "m2 = 'phi3'\n",
    "\n",
    "class LLMInterface:\n",
    "    def __init__(self, model_name=m2):\n",
    "        self.model_name = model_name\n",
    "        self.api_url = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "    def generate_response(self, prompt, max_tokens=500):\n",
    "        data = {\n",
    "            \"model\": self.model_name,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": False,\n",
    "            \"max_tokens\": max_tokens\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(self.api_url, json=data)\n",
    "            response.raise_for_status()  # Raise an exception for bad status codes\n",
    "            return json.loads(response.text)['response']\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            return f\"Error: Unable to generate response. {str(e)}\"\n",
    "\n",
    "    def simple_query(self, question):\n",
    "        prompt = f\"Human: {question}\\n\\nAssistant:\"\n",
    "        return self.generate_response(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Flux de travail (Main Script) : `main.py`\n",
    "\n",
    "### Fonction `format_context(results)`\n",
    "- Formate les résultats de la recherche pour le prompt du LLM.\n",
    "\n",
    "### Fonction `main()`\n",
    "- Initialise les composants (SemanticSearch, LLMInterface).\n",
    "- Gère la boucle principale d'interaction avec l'utilisateur.\n",
    "- Coordonne le processus de recherche, génération de réponse et affichage.\n",
    "\n",
    "**Techniques clés** :\n",
    "- Orchestration du flux de travail complet du chatbot.\n",
    "- Formatage du prompt pour optimiser la réponse du LLM.\n",
    "- Affichage des sources pour la transparence.\n",
    "\n",
    "_Note : L'interface streamlit(`streamlit_app.py`) reproduit à peu près le même processus._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot de la Constitution française\n",
      "-----------------------------------\n",
      "Posez vos questions sur la Constitution française. Tapez 'q' pour quitter.\n",
      "\n",
      "Réponse du chatbot :\n",
      " Le Président de la République nomme le Premier ministre, comme indiqué dans l'extrait 1 de la Constitution française.\n",
      "\n",
      "Sources utilisées :\n",
      "1. Titre II \n",
      "LE PRÉSIDENT DE LA R ÉPUBLIQUE - Article 8\n",
      "2. Titre III  \n",
      "LE GOUVERNEMENT - Article 21\n",
      "3. Titre II \n",
      "LE PRÉSIDENT DE LA R ÉPUBLIQUE - Article 13\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def format_context(results):\n",
    "    context = \"\"\n",
    "    for i, result in enumerate(results[:3], 1):  # Utiliser les 3 premiers résultats\n",
    "        context += f\"Extrait {i}:\\n\"\n",
    "        context += f\"Titre: {result['metadata']['title']}\\n\"\n",
    "        context += f\"Article: {result['metadata']['article_num']}\\n\"\n",
    "        context += f\"Contenu: {result['content']}\\n\\n\"\n",
    "    return context.strip()\n",
    "\n",
    "def main():\n",
    "    pdf_path = \"constitution.pdf\"\n",
    "    \n",
    "    '''# Traitement du PDF (si ce n'est pas déjà fait)\n",
    "    processor = PDFProcessor(pdf_path)\n",
    "    chunks = processor.process()\n",
    "\n",
    "    # Génération et stockage des embeddings (si ce n'est pas déjà fait)\n",
    "    embedding_gen = EmbeddingGenerator()\n",
    "    embedding_gen.process_and_store(chunks)'''\n",
    "\n",
    "    # Création des objets de recherche sémantique et d'interface LLM\n",
    "    searcher = SemanticSearch()\n",
    "    llm = LLMInterface()\n",
    "\n",
    "    print(\"Chatbot de la Constitution française\")\n",
    "    print(\"-----------------------------------\")\n",
    "    print(\"Posez vos questions sur la Constitution française. Tapez 'q' pour quitter.\")\n",
    "\n",
    "    while True:\n",
    "        query = input(\"\\nVotre question : \")\n",
    "        if query.lower() == 'q':\n",
    "            break\n",
    "\n",
    "        # Recherche sémantique\n",
    "        results = searcher.search(query, n_results=3)  # Récupérer les 3 meilleurs résultats\n",
    "\n",
    "        # Formater le contexte pour le LLM\n",
    "        context = format_context(results)\n",
    "\n",
    "        # Préparer le prompt pour le LLM\n",
    "        prompt = f\"\"\"En tant qu'expert en droit constitutionnel français, veuillez répondre à la question suivante en vous basant uniquement sur les extraits fournis de la Constitution française. Si les extraits ne contiennent pas suffisamment d'informations pour répondre à la question, indiquez-le clairement.\n",
    "\n",
    "Extraits de la Constitution :\n",
    "{context}\n",
    "\n",
    "Question : {query}\n",
    "\n",
    "Réponse :\"\"\"\n",
    "\n",
    "        # Générer la réponse avec le LLM\n",
    "        response = llm.generate_response(prompt)\n",
    "\n",
    "        print(\"\\nRéponse du chatbot :\")\n",
    "        print(response)\n",
    "\n",
    "        # Afficher les sources utilisées\n",
    "        print(\"\\nSources utilisées :\")\n",
    "        for i, result in enumerate(results, 1):\n",
    "            print(f\"{i}. {result['metadata']['title']} - Article {result['metadata']['article_num']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pistes d'optimisations futures :\n",
    " \n",
    "* Optimisation de la gestions des ressources pour accroître la vitesse de réponse du modèle.\n",
    "* Optimisation des mécanismes de traîtement(chunks, embeddings) et des méthodes de calculs de similarités pour améliorer les performances lors des questions plus complexes.\n",
    "* Améliorer les mécanismes de gestion contextuelle (Prompt engineering plus avancé).\n",
    "* Intégration des mécanisme de gestion des exception (Par exemple, pour que le modèle réponde correctement lorsqu'il ne trouve pas l'information ou lorsqu'elle est absente).\n",
    "* Amélioration de l'interface graphique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
